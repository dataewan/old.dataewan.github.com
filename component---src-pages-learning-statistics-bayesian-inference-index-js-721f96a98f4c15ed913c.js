(window.webpackJsonp=window.webpackJsonp||[]).push([[8,11],{R93W:function(e,t,a){"use strict";a.r(t);var r=a("q1tI"),n=a.n(r),i=a("+ZDr"),l=a.n(i),o=a("Kvkj"),s=a("lXgu"),c=a("rwhV"),u=a.n(c);t.default=function(){return n.a.createElement(o.e,null,n.a.createElement(o.j,{name:"Bayesian inference"},n.a.createElement(o.h,null,"Bayesian inference is a way of drawing statistical inferences from data. You make a model with parameters θ. You then use Bayes rule to get the probability distribution for those parameters after we observe data."),n.a.createElement(o.d,{caption:"Bayes rule for the probability of θ given data"},n.a.createElement("img",{src:u.a})),n.a.createElement(o.h,null,"There are four parts to this equation.",n.a.createElement(s.default,null)),n.a.createElement(o.h,null,"Bayes rule gives you a way to get from ",n.a.createElement("i",null,"p(data | θ)")," to"," ",n.a.createElement("i",null,"p(θ | data)"),", flipping round the equation to give you the posterior. An alternative way of thinking about it is that it gives you a way to update your beliefs after you observe new data."),n.a.createElement(o.h,null,"We go to a lot of hard work to calculate the posterior distribution, which is what the data tells you about the parameters of your model. From the posterior you can make summaries of your analysis to state the likely values of the parameters of the model, and you can also make predictions from the posterior."),n.a.createElement(o.k,{name:"Sampling to get the posterior"},n.a.createElement(o.h,null,"The posterior is really hard to calculate exactly, because the evidence term is usually a horrible integral. We take advantage of something we found in"," ",n.a.createElement(l.a,{to:"./learning/statistics/bayesian-inference/evidence/"},"the section about evidence")," ","to rewrite Bayes rule as a similarity relation."),n.a.createElement(o.h,null,n.a.createElement(o.c,{equation:"$p(\\theta | data) = \\frac{p(data | \\theta) p(\\theta)}{p(data)}$"}),n.a.createElement(o.c,{equation:"$p(\\theta | data) \\propto p(data | \\theta) p(\\theta)$"})),n.a.createElement(o.h,null,"This trick lets us use a family of techniques called ",n.a.createElement(l.a,{to:"./learning/statistics/bayesian-inference/mcmc/"},"Markov Chain Monte Carlo sampling"),". These techniques let us get a very good approximation of the posterior distribution, and allows us to solve real world problems."))))}},lXgu:function(e,t,a){"use strict";a.r(t);var r=a("q1tI"),n=a.n(r),i=a("+ZDr"),l=a.n(i),o=a("MGXT");t.default=function(){return n.a.createElement("div",null,n.a.createElement("div",{style:{border:"solid 5px "+o.a.lightyellow,padding:10}},n.a.createElement(l.a,{to:"/learning/statistics/bayesian-inference/likelihood"},n.a.createElement("h3",null,"Likelihood — p(data | θ) "),n.a.createElement("p",null,"Given our model, how likely is it that we would observe this data?"))),n.a.createElement("div",{style:{border:"solid 5px "+o.a.lightblue,padding:10}},n.a.createElement(l.a,{to:"/learning/statistics/bayesian-inference/prior"},n.a.createElement("h3",null,"Prior — p(θ) "),n.a.createElement("p",null,"Before doing the inference, describe what you think the parameters are by specifying the priors."))),n.a.createElement("div",{style:{border:"solid 5px "+o.a.lightgreen,padding:10}},n.a.createElement(l.a,{to:"/learning/statistics/bayesian-inference/evidence"},n.a.createElement("h3",null,"Evidence — p(data) "),n.a.createElement("p",null,"The denominator of Bayes rule. I've also seen this called the"," ",n.a.createElement("i",null,"marginal probability"),"."))),n.a.createElement("div",{style:{border:"solid 5px "+o.a.lightred,padding:10}},n.a.createElement(l.a,{to:"/learning/statistics/bayesian-inference/posterior"},n.a.createElement("h3",null,"Posterior — p(θ | data) "),n.a.createElement("p",null,"This is the thing that we're trying to calculate. What are the values of θ that we should use for our model?"))))}},rwhV:function(e,t,a){e.exports=a.p+"static/bayes_rule-94c052427fc2797095ddd09ac553e974.png"}}]);
//# sourceMappingURL=component---src-pages-learning-statistics-bayesian-inference-index-js-721f96a98f4c15ed913c.js.map